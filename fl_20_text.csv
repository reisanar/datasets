due_date,comments
document_01,"I enjoyed the way she explained and differentiated support and confidence, in that support is based on the probability of both sets occurring together, while confidence is the probability that those items occur together given that the person has the same set of items X already themselves. "
document_01,"Here is a research proposal paper about the Application of Data Mining Techniques to Find Relationships Between the Dishes Offered by a Restaurant for the Elaboration of Combos Based on the Preferences of the Diners.  ( )The research paper presents the implementation of a system that serves as support to help elaborate comb preferences diners have by using data mining and association rule mining to find key relationships based on what they frequently order. Some of their research included using the Apriori Algorithm, which presented the options of a list of suggested combos of the selected restaurant and the list of dishes that are sold together with a dish that the diner has selected. In reading the conclusion, the authors decided that after investigating multiple methodologies, using the Apriori Algorithm appeared to be the best, and that ""the algorithm yield(s) the frequent items within a transaction base, which guarantees that the combos suggested by the algorithm are obtained based on the preferences of the diners, acting in the same way to obtain the elements that are frequently acquired with a selected element."""
document_01,The way she explained how transactions even on a large scale can be mostly predictable was very interesting to me. the large web diagram that she showed with all the products a person was buying and how each correlated or did not correlate with each other was an interesting practice that I didn't know supermarkets used.
document_01,"The research paper I choose is ""Pushing support constraints into association rules mining"". https://ieeexplore.ieee.org/abstract/document/1198396 ( )"
document_01,"This document is about using association rules mining to examine support constraints and improve you businesses support system. By looking at the patterns at the different levels of support appear we can understand how to improve our support systems. "" Our approach is to ""push"" support constraints into the Apriori itemset generation so that the ""best"" minimum support is determined for each itemset at runtime to preserve the essence of Apriori."""
document_01,"As someone who actually enjoyed Statistics (and was also my better performing math classes) I very much like how she presented as the transaction data as essentially probability, in which it pretty much is.  Which makes it much easier for my head to understand and then jump into the newer jargon such as Support and Lift."
document_01,"A paper that I have found called Privacy-preserving association rule mining for horizontally partitioned healthcare data: a case study on the heart diseases goes about to help in making association rule mining in Electronic Healthcare Records (EHRs) abide by the confidentiality laws that are in place with health records.  Because in order for association rule mining to work in healthcare, data will need to be shared but in doing so would break patient confidentiality.  Their solution is to implement Privacy-preserving Distributed Association Rule Mining (PPDARM), with a secure algorithm on horizontally partitioned data with the added ability to be extended to vertically partitioned EHRs."
document_01,https://link.springer.com/article/10.1007/s12046-018-0916-9 ( )
document_01,great post!
document_01,"The primary thing I love about these data presentations is how much twitter has become a big part of them. For some reason, Twitter has become a super easy and interesting dataset that can change daily and has millions of records to access. It provides a great platform for examples like the one in the presentation. "
document_01,"I find ARM interesting, and so far we have only used APRIORI. I looked up papers about different methods and found this paper by ISSN from Research India Publications. ( ) To say I was overwhelmed was an understatement! The paper essentially summarizes and looks at a bunch of ARM methods and algorithms and discusses their invention. There are tons of different methods like DHP which is a hash based version of APRIORI that aims for efficiency. There is also a newer algorithm from 2005 called Recursive Elimination which makes a linked list to simplify the process and visualization. Another interesting point the paper brought up is that some algorithms were built specifically for niche implementations. "
document_01,"I very much enjoyed the video, I found it extremely informative and relatable . I thought that the way that Dr. Gates presented and worded the way that association rule mining works allowed for greater utilization of the process.  I also found it extremely relatable in her discussion of the relationship that one needs to make with R Code in data analysis. The ability to calculate lift associated with a group of variables allows for the researcher to better understand which variables (and by association their superscripts) have the most association with others. "
document_01,"Share the name of the paper, a short description of the main results (check the abstract and conclusions of the paper), and a direct link to the work."
document_01,"The article that I found interesting in the field of association rule mining is titled: ""Application of Association Rule Mining and Social Network Analysis for Understanding Causality of Construction Defects ( )"" posted by MPDI in 2019. The goal of the analysis was to use Association Rule Mining and social network analysis to log and categorize the causality of defects that occur on a construction site. The suggested approach was applied to 2949 defect instances that were logged through the completion of a project. Through this application, it was confirmed that the proposed approach can systematically identify and quantify causality among defect causes."
document_01,I found the presentation very interesting. It was a great review of the basics of association rule mining and some practical applications of it. Being able to find rules among tweets or among the contents of your grocery basket really shows how you can use ARM for many different things. I also liked how she talked about how she cleaned the data since it is really important but not always mentioned.
document_01,AR-ANN: Incorporating Association Rule Mining in Artificial Neural Network for Thyroid Disease Knowledge Discovery and Diagnosis. ( )
document_01,This paper describes a use of Association Rule Mining for thyroid disease analysis. ARM is used to determine which qualities of a patient contribute to thyroid disease. The study shows how the rules created shows that age and sex are the most important characteristics for determining whether someone has thyroid disease. The knowledge generated by these rules can then be feed into a neural network. It is quite interesting how Association Rule Mining is used as a type of dimensionality reduction by showing what features are most important for diagnosing thyroid disease.
document_01,I liked seeing Ami Gates' illustrations of moving from Basket to Single to Sparse Matrix transaction data representation. It was worth noting that she said transaction ID numbers are unnecessary. It is clear from her presentation that other applications of transaction data require a lot of data formatting and cleaning.
document_01,Discovering the relationships between yarn and fabric properties using association rule mining ( ) from the Turkish Journal of Electrical Engineering & Computer Sciences
document_01,"The research paper explores the effects of yarn parameters on fabric quality and also finding important parameters to achieve desired fabric properties. Specific properties of materials are known, but as the authors state ""Data mining plays an important role in discovering hidden patterns from fabric data and transforming it into knowledge."" The researchers used the Apriori algorithm, frequent pattern growth, and Equivalence Class Clustering and bottom-up Lattice Traversal (Eclat). They applied these algorithms to a yarn and fabric dataset to discover relationships among a set of yarn parameters (i.e. hairiness, capillary, and diameter) and fabric properties (i.e. pilling, wrinkle, and bending rigidity). The study's experimental results discovered a set of association rules that demonstrate that the association rule mining method is a viable approach to discovering rules on textile data. The authors mention that they have more areas to investigate, such as negative rules, and the effectiveness of their novel approaches such as cf-item and mf-item to perform single item-based data analysis."
document_01,"Something I found very helpful was the way she explained support, confidence, and lift. As she said in the video, the examples helped me ""feel"" what each of those means. I also found it decently funny that the data in one of her slides said ""chocloate"" instead of ""chocolate."" Making tweets into transactions was also very interesting to me. It makes me think about possible applications beyond the items people buy together at stores. Very cool!"
document_01,"After a bit of searching, I found a paper on a topic that is both interesting and something I didn't know existed up to this point: the intersection of data and coffee. I worked for a local shop called Concord Coffee for a year before quitting to do research at the university, along with some other shops for a few years before that, and coffee still holds a very special place in my heart. In ""Correlating Espresso Quality with Coffee-Machine Parameters by Means of Association Rule Mining,""  ( )Apiletti and Pastor explore the quality evaluation of espresso through association rule mining, rather than the traditional method of expert tasting. They validated the rules you learn while training as a barista, such as balancing the amount of coffee, the grind settings, and machine pressure. The study found that the most important external factors outside of industry-level factors (coffee type, roast characteristics, etc) are directly controlled by the barista. For instance, pressure (a factor determined by the machine and often not easily changed) is much less important than the grind settings and coffee amount (two things that can be changed each shot). An interesting, but not directly related, statistic that was brought up was the fact that 97% of Italian adults have at least one shot of espresso every day. There were a lot of other interesting finds, but I'm not going to bore everyone by ranting about coffee."
document_01,I really liked learning about the arulesViz graphic she used. I didn't realize you could make an interactive graph in R. It very clearly shows the support and confidence of each association.
document_01,http://www.ijsrp.org/research-paper-0714/ijsrp-p3158.pdf ( )
document_01,"This article shows how association rule mining techniques can be used to improve the field of agriculture. Some examples are, collecting summary information about crop production to make algorithms to help with decision making. This can help the farmers identify the different crop losses and prevent them in future. Also, researchers are designing an effective risk management system using extreme rainfall events and climatic indices to trace frequent occurrences of droughts and floods. "
document_01,I really enjoyed this presentation.  Two things I took away from this lecture was the term interestingness and also that the Apriori Algorithm handles pruning and efficiency very similar to the AI Game Theory Minimax with Alpha-Beta pruning.  
document_01,I found an interesting research paper combining both association rule mining and Dota 2.
document_01,https://www.researchgate.net/publication/328434167_Analysis_of_Players_Transfers_in_Esports_The_Case_of_Dota_2 ( )
document_01,This paper is an analysis using association rule mining to see relationships between pro Dota 2 players moving to other teams.  It was found two of the biggest reasons were the region of the world the player was from and if they were a TI(The Grand Championship of Dota) winner or not.
document_01,"I found the part where Dr. Gates mentioned people had created a list of the happiest cities based on tweets from each city using geo-data very interesting. I never would have guessed Boulder, Colorado would be one of the happiest. I wish she would have talked a little bit more about the group’s methodology for that experiment. "
document_01,"There was an economic study done in 2012 at the Hong Kong Polytechnic University that used association rule mining to determine the effects of global market indexes, GDP, and other economic indicators have on the Hang Seng Index. The result was finding a satisfactory model generated by association rule mining, so this study, named “Using a fuzzy association rule mining approach to identify the financial data association”, ended up combining association rule mining with statistical learning methods! "
document_01,Link: https://www.sciencedirect.com/science/article/pii/S0957417412002916 ( )
document_01,"This was a great talk overall! I loved her energy, and it kept me engaged from start to finish. Her review of what association rule mining was really good to bring together a lot of concepts we have seen in class. It was interesting to hear about how she cleaned the data from twitter/twitteR and how to use association rules for things that you might not traditionally think of as ""transactions."" Cleaning always seems like the hardest part of data analysis. "
document_01,"I found this article ( ) on how association rule mining was used to suggest that owning pets (particularly cats and dogs) likely reduces the risk of developing heart disease. It's something I have heard of in the past, but always assumed that this study was conducted via blood pressure cuffs and large groups of people, not the Apriori algorithm. It was interesting to see that the outcome wasn't necessarily that animals lowered stress (what seems to be the most common explanation on the topic), but a large contribution is that animals force people to stay more active, mentally and physically. "
document_01,"The presenter was clearly very passionate about R and association rules, which made the video easier to understand and stay with. She had some very helpful visuals, such as the arulesViz plot about chocolate and the related words."
document_01,"My only personality trait is liking baseball, so I tried to find something about baseball. Unfortunately, I could not find anything. However, I did find this ( ) paper titled ""Applying Association Rule Mining to analyze the performance of Indian Cricket Team in T20"". I don't know anything about cricket, but it seemed close enough. The authors of this paper used association rule mining to try and find factors that could lead to winning games. One thing they found was that when India bats first against a team called SI and scores less than 150 runs, they usually lose. They also found that India performs better in the T20 cricket format than the ODI format. They provided this and more information to the managers of the Indian team so that they could possibly improve their performance. Being still kinda lost in this subject I don't know what else to pull from this but I'm sure it's full of interesting stuff!"
document_01,"When watching the video, she reminded me a lot of the way Dr. Skiles teaches, which I loved. She was very informative and I liked how she was honest with arules, saying everyone will struggle. Her arules graphic was awesome to see visually."
document_01,Article ( )
document_01,"The article I found was called, ""Analysis of motorcycle accidents using association rule mining-based framework with parameter optimization and GIS technology"". The author of this article used association rule mining (ARM) to identify the critical factors associated with the seriousness of injuries in motorcycle accidents. They identified that there are limitations from existing studies on the subject in regard to objectiveness and efficiency, and that they lacked in-depth analysis. They applied their improved method to data from Victoria, Australia. More so, they identified several factors related to fatal injuries, and were able to use Geographic Information System (GIS) to identify where fatal accidents happen more."
document_01," I liked the video, where she gives a brief introduction of association rules to introduce it to her audience. Still, the point that most caught my attention was how it was applied to analyze Twitter data. The speaker goes through many of the steps that she had problems with, starting from getting the data from Twitter using a developer's account, then proceeding to the cleaning process where she had to use grep to take out digits, https, and things that could yield problematic associations to the analysis. It also stood out when she mentioned some points she had to clean the data manually and how interesting the data visualization looked like."
document_01,"Here is a research proposal paper about Identifying Influential Users on Twitter’s Trendy Hashtags Using Association Rule Learning ( ) that brings a brief explanation of how association rule mining was applied to trendy hashtags. In this study, some key elements, such as the support, confidence, and lift values from different hashtags related to Christmas, were used to identify the factors associated with the evolution of these hashtags and the effects of influent users. I found it really interesting since it is directly connected with the subject of the speaker."
document_01,"Awesome article, Gustavo!"
document_01,I thought that this presentation was very informative and helped to show the various steps for association rules mining in R in a condensed way. I feel like this video is an easy way to help someone grasp the concepts as well as being useful for reviewing the concepts.
document_01,The paper I found was related to creating a genetic algorithm to mine for the association rules of a given set as opposed to apriori and other algorithms and utilizing the FP-tree algorithm to improve the efficiency of the algorithm. This is the link to the paper ( ). This paper was very interesting to me because it is a novel use of a type of computational learning which is helping to optimize various fields.
document_01,"I really enjoyed this presentation, she did a wonderful job breaking association rule mining. I thought it was really interesting to see her explain her cleaning techniques from something as unpredictable as twitter. I also enjoyed that she said manually cleaning parts of data wasn't cheating or a taboo topic."
document_01,"I found a paper called ""A study of IoT malware activities using association rule learning for darknet sensor data ( )"" that talks about using association rule learning to find patterns on the darknet of hackers trying to compromise Internet-of-Things devices. They successfully used it as an early detection program for threats on IoT devices. This could be used as an autonomous cybersecurity measure for these devices."
document_01,I enjoyed Dr. Gates explaining how the association rule mining and apriori algorithm works. It gave me a better understanding of how the two topics are utilized. What tied everything together was when she talked about the transaction data that she created and walked through the process of how she incorporated the arule library and apriori algorithm using R.
document_01,"This interesting article  ( )titled, "" Association Rule Mining Tourist-Attractive Destinations for the Sustainable Development of a Large Tourism Area in Hokkaido Using Wi-Fi Tracking Data"" talks about utilizing association rule mining to observe the tourism area in Hokkaido to look for ways to develop and enhance sustainable tourism attractions while maintaining the natural landscape. The authors worked with transaction data and association rule mining to study the patterns of what destinations were frequent the most and if the attraction had a part in that. In this study, 31 destinations in Hokkaido were studied and multiple techniques were used to evaluate the rules of the destination visits."
document_01,This was a pretty cool presentation! The speaker had a pleasant sense of humor. I enjoyed learning about how the measures of correlation can be formatted in R. Also the visualization of the concepts of the apriori algorithm provided more insight to what we’ve been learning in class. She did a great job unpacking those rules!
document_01,"I found this article ( ) interesting. It explores the accuracy of identifying hypertension, a common cardiovascular disease, among patients using class association rules. The CARs gather meaningful information from patients about their physiological status. The researchers also designed a set of features that characterize significant patterns while analyzing patients."
document_01,"I like how she organized the slides that can be understood by a novice, especially when she color-coated the symbols for the {diaper} --> {beer} example. It's good to create slides that are comprehendible to those who have limited knowledge of the topic, which is apparent in industries."
document_01,"I found this article, https://www.cs.purdue.edu/homes/clifton/DistDM/kdd02.pdf  ( ) discusses the issue of association rule mining where transactions are distributed across sources. The paper focuses on how sites hold some attributes of every transaction, and why these sites would like to cooperate to distinguish universally accepted association rules, but the sites must not show the truth of the individual transaction data. Their main goal is to develop methods that enable any data mining that can be achieved at a sole site to be done across many sources while obeying the given privacy policies. The paper reveals that it is achievable to receive quality individual security while communication costs correspond to the required build of a centralized data warehouse. "
document_01,"What interested me about the presentation was that she determined that transaction information was essentially probability, which helps statisticians to make sense of the data they are analyzing."
document_01,I found a research paper analyzing motorcycle accidents using association rule mining-based framework with parameter optimization and GIS technology.  ( )
document_01,The paper is interesting and points out the factors that may be related to fatal accidents. They use GIS maps to present hot spots of motorcycle accidents related to fatal factors.
document_01,"Like everyone else I was super surprised about Beer and diapers being associated. The Core concept seems to be correlation of two things happenining. Does not explain why, but shows if A then B.  Which a lot of times you dont need to explain why things happen togther, you just need to know that they do. I found this article interesting."
document_01,https://getpocket.com/explore/item/think-you-have-a-hit-make-sure-it-s-the-first-song-on-your-album?utm_source=pocket-newtab ( )
document_01,"This article talks about the association of skip rates and intro lengths. Basically associates having your better songs first in an album means more people listen to it, thus more plays and money. They call this ""front loading"" an album."
document_01,"Her presentation was really interesting and I enjoyed how she involved the audience consistently. Also, the examples she used to explain the content were funny and represented real situations (I never thought about how diapers and beer would be correlated when shopping but it actually makes sense). "
document_01,Article's name: Using Association Rules Mining for Retrieving Genre-specific Music Files  
document_01,Link: https://www.researchgate.net/profile/Ismail_Biskri/publication/317338780_Using_Association_Rules_Mining_for_Retrieving_Genre-specific_Music_Files/links/593375f645851553b6ca8a46/Using-Association-Rules-Mining-for-Retrieving-Genre-specific-Music-Files.pdf ( )
document_01,"This article is a study about how Association Rules Mining could help with searching for specific music files within a large database, since it is a tedious and complex task. They suggested using association rules mining to find significant relations between content-based descriptors of musical files and use these relations to create indexes. Within their conclusion it is possible to assume that association rules can yield interesting results especially for retrieving classical music, and even though the study is focused on music, this methodology can be also applied to image, video, or text."
document_01,"I thought that in Dr. Gates initial introduction about the correlation of diapers and beer being purchased simultaneously was interesting because it's a humorous descriptor of the human condition, but while researching paper I found another research paper preview on a similar topic regarding the political tweet mining that she conducted as well. It was a paper done by Lakshmi Prasanna Konudula on the topic of Leveragaing Social Media to Discover Threatening Tweets Using Clustering and Association Rule Mining; however, this was not a full access article, but I did find it interesting that Konudula utilized the profanity as a marker for Tweets of this nature, rather than Dr. Gates who took the time to remove them. I can confirm that Boulder, Colorado is one of the happiest places in the U.S. I highly recommend hiking Royal Arch ( ) in the area. "
document_01,In terms of full length research proposal papers A Systematic Review of Defensive and Offensive Cybersecurity with Machine Learning  ( )utilized many techniques that we are studying in this course. 
document_01,"This is a systematic review of over one hundred research papers about machine learning methods applied to defensive and offensive cybersecurity. In contrast to previous reviews, which focused on several fragments of research topics in this area, this paper systematically and comprehensively combines domain knowledge into a single review. Ultimately, this paper seeks to provide a base for researchers that wish to delve into the field of machine learning for cybersecurity. Our findings identify the frequently used machine learning methods within supervised, unsupervised, and semi-supervised machine learning, the most useful data sets for evaluating intrusion detection methods within supervised learning, and methods from machine learning that have shown promise in tackling various threats in defensive and offensive cybersecurity. "
document_01,"Offensive Machine Learning Techniques: Neural networks were the commonly used technique, while unsupervised ML techniques such as association rule mining, frequent pattern mining, and clustering were used in a few studies. Combinations of supervised methods as semi-supervised learning were also used."
document_01,"I enjoyed watching this video as it provided me with much insight into how Association Rule Mining is so important when working with data. While watching this video, the part I found interesting was that each Tweet must be treated as its own transaction. That appears to be super helpful when using Association Rule Mining since there is so much information in each tweet, a lot can be interpreted and analyzed from the data.  "
document_01,"The article I chose is “Association rule mining based quantitative analysis approach of household characteristics impacts on residential electricity consumption patterns”, and I found it interesting because it not only showed residential electricity consumption patterns, but also the reasons behind them. These reasons are considered household characteristics and show us much about the data. Some of the conclusions are that if a house uses electricity to cook, they will have higher electricity consumption. Another conclusion is that households with at least two people will have a bigger fluctuation in electricity consumption between seasons. Overall, I found this to be a great resource that explains how useful Association Rule Mining for interpreting data."
document_01,https://www.sciencedirect.com/science/article/pii/S0196890418306265 ( )
document_01,"One thing that I really liked about this presentation is how she went about showing the apriori algorithm using the visual aids. I also ultimately think that without the visual aids it would be pretty difficult to understand some of this content. I found this really interesting article on ieee https://ieeexplore.ieee.org/abstract/document/7374211 ( ) ""Analysing road accident data using association rule mining"" Its a really interesting article where  they use association rule mining to look at accident data not only on major highways but also on Local surface streets. They used the results of the study to determine which type of accidents were happening the most and then they would use this to suggest possible solutions to the problem.  It was association rule mining that ultimately led them to fixing the amount of accidents that were happening in certain areas. "
document_01,"After watching the presentation, I found it very interesting specifically the topic about Lyft and that Lyft is a really cool way to determine whether or not something is interesting through the interesting rule set. Weather a rule shows if something is interesting or not. I thought that was a very cool topic that I would like to use more in my studies."
document_01,"The article I found was using Association rule mining to analyze customers for banks and see what type of customers they have and how they can use that data to get more customers coming through their banks. Specifically, Using a two stage framework of the customers behavior to analyze and create an algorithm which is used to identify groups of customers that are based on their recency, frequency, and monetary behavior scoring predictors that divides the customers into three profitable groups. Then that is used to create customer profiles by identifying them by their behaviors to help facilitate market strategies and development for the actual bank."
document_01,Link for the article: http://ijiepr.iust.ac.ir/article-1-241-en.html
document_02,"Something I found interesting in this lecture was his comparisons of data analytics to other subjects. He showed how theories in music control what will sound good and what wont, so we use the same logic to apply theories to data analysis to show what will look good and what wont. In his music example, he has many issues with his lines of music; it had missing notes, a direct octave, unresolved 7th, parallel fifths, which all violate the music theory. This violation indicates that this set of notes will not sound good. In terms of data analysis, once a theory is found, it could be infinitely scalable to help teach data analysis and could be applied to multiple visualization problems. If the figure is not effectively created, it will not show the comparisons, causality, mechanism, systematic structure, multivariate data, etc. This theory can be applied as a blanket theory to deter from ineffective data graphics. "
document_02,"I loved the use of music to illustrate different points in this talk! I'm not really a musical human being, but the examples were so clear through this train of thought. It was really cleverly done and even I could appreciate. :)"
document_02,"My favorite part was also definitely his discussion was the Charles Ives/Avril Lavinge/etc. segment discussing music. I find music extremely fascinating both as someone who plays and someone who finds the concept of data and/or machine created music fascinating. Is what a machine creates truly art? If a machine can create art, have we lost one of our greatest distinctions from them? Part of what creates music is breaking the rules that define it."
document_02,"While it may have been semi-obvious, I did look up to confirm that the Tidy Package directly connected to the Tidyverse we still use today. Crazy to think just how groundbreaking it was. I was a little disappointed that he was talking about music as a methodology for creating better code rather than talking about using data to drive musical creation, but I do think the association is extremely strong."
document_02,"The Aesthetics -> Methodology segment I think was my favorite part of this chunk of the lecture. Thinking about code AS a language I find fascinating conceptually, and I completely agree that trying to create similar code ‘Aesthetics’ is potentially really important for expanding the field, especially with regard to using the tools we have better, rather than just constantly improving the tools themselves.  "
document_02,"Being the fact that I am majoring in Data Science, I have heard the statement that data analysts are needed and the demand for analysts are only increasing.  However, I never really thought about automation, even though like Dean said, its one of the first thing people think about changing or improving a job field.  Cause it happens with so many others so why not data analysts?  And I just started thinking, can data analysts or jobs related to it become fully automated?  We have software and programs that help us calculating and presenting data but these are really seen more as tools than anything else, at least in my point of view.  Or will the data analysis field of work always need some kind of human element, and I don't mean as like a technician or operator, but as someone that interprets the data.  I feel that at least right now, yes, we need a human element.  Even though we can make plenty of mistakes, we can also make intuitive decisions that programs may not be capable of yet.  Maybe in the future it can become automated, and would be interesting to see if its within our lifetime."
document_02,"This leads to interesting discussion... for now, never forget the power of intuitive decisions, creativity, and assessment"
document_02,"I liked the quote he had at around the 19-minute mark. It said ""Statisticians have  a process they espouse but do not know anything about."" It reminds me of Calc 2 with Professor Burbank. Nobody feels like we needed to know how or why things (like integrals) are calculated a certain way since we have calculators that can do it for us, but if we didn't learn it, who would make new calculators? I don't know if that's exactly what the quote is saying but I'll roll with it. In terms of baseball, I can know what every stat means and what a good number to have in that column is, but if I don't understand the game-mechanics, biometrics, and other key factors that go into those numbers, my analysis of them probably won't be that insightful. I just want to talk about baseball ;_;"
document_02,Him talking about music theory gave me unwelcome flashbacks. 
document_02,Game 1 Tuesday night. More and more jobs like this one in upcoming years: R&D Analyst for the Mariners  
document_02,"I'm a Padres fan but since I live basically in Tampa I'll be rooting for the Rays! I also just applied for a position with the Rangers, but it said you need a degree so I'm not too sure about it :3"
document_02,"Overall, in the video, the presenter was very entertaining. I enjoyed his comments and found them excellent. When watching the video, I thought it was interesting that he mentioned people find it challenging to define data analysis. My thoughts on data analysis were that it was all the items he listed. I do not think we will ever have a general definition of data analysis. When you look at data analysis job descriptions, they are specific. You have to know different programming languages; however, the different jobs' summary can be entirely different from each other. Whereas in other jobs such as a research analyst, the summaries are similar. This was just something I noticed when applying for internships.  I found it amusing that he wrote his own music example and knew how it sounded when he wrote it. Additionally, I learned about who named tidy data. It made me reflect on how it is used for homework. "
document_02,Dr. Peng's lecture was great to watch and very insightful. He provided many applications and examples to explain what data analysis is and how it works. His approach on the word research versus re-search was interesting.
document_02,"When he talked about music theory, I thought it was a great approach to help people understand the process of data analysis. Explaining and showing the different music genres was a fun and easy way of explaining that once you know the basics and fundamentals of music theory (or data analysis), you can predict what will come after next note (reviewing a peer's work) or produce something awesome from scratch (creating your own EDA or models). I think this holds true because I already know the methods and different tools to use to produce models and analysis. I can discern if my models have any discrepancies or came out the way it should and make any changes based on that.   "
document_02,"The examples that he discussed towards the end are good reminders when working with data analysis: reproducible for people, translatable to other software, and robustness to new data."
document_02,"“Terrible plots come from excel, good plots come from everywhere else”"
document_02,I agree that including music theory is a great way to describe the data analysis process. Even if you have no experience in this field I feel like you would still come away from this lecture with a decent understanding because of the way he integrated the simple music theory into his lecture.
document_02,"""reproducible for people, translatable to other software, and robustness to new data."" - excellent!"
document_02,I really enjoyed this presentation by Dr. Roger Peng and i found it very informative.
document_02,Some points I found interesting from this hour long presentation were:
document_02,I really liked how he went about comparing data analytics to other subjects that you might at first not relate.
document_02,I found it really interesting to hear about some of the challenges that Roger had to face. I really related to the point where he was talking about the student who was a good student and by the second to last lecture they were saying they understand how to do everything conceptually but they don't know how to apply it. 
document_02,I found Roger while very accomplished in his own regard someone very easy to relate to with the way he talks about subjects and the way he interacts with the audience. Much like another data science professor I have right now.
document_02,Towards the end of the presentation I really enjoyed the links to music he was making when talking about data analytics. 
document_02,"Definitely felt for that student, I have been there where I think I understand a topic but don't know how to apply, only to learn from a professor that I really don't understand that topic at all then. "
document_02,"I found it interesting how he used sheet music as an example of Data analysis, as someone who played the trumpet back in high school i saw sheet music all the time and never thought of it in the way he describes as compiled data. I makes sense since sheet music is composed of many notes and added thing to change the dynamics, Just as a data set or database is made from many different individual data points which are collected together and extrapolated into graphs and data that paint a bigger picture. It was nice to see something i was so similar with being used to explain data analytics, the fact that he had many examples like John Coltrane and Gustave Mauler both of which i have been exposed to in my High school band days. It hammers home the point of data analytics which is to bring all these pieces together and find a meaning to it."
document_02,That is awesome that you played the trumpet! I did not know that. I like your analysis on how you have been exposed to both those types of music. I personally have never heard of both those artists. 
document_02,"This was a very interesting lecture. Dr. Peng brought some great points that I have never thought about before when it comes to data analysis. The biggest point that stood out to me is how we never really have been taught the ""rules"" of a good data analysis. We are taught statistics, modeling techniques, and packages but never a framework for analyzing our data. I love how he uses the field of music as an example and how music theory gives a foundation for making good music. Another example is color theory, which gives a framework on how to use colors effectively for visual designs. We just don't have that theory or foundation yet for data analysis but it will be extremely beneficial for society once we do. One of the audience members brought up an interesting point about individuals who will break the ""rules"" of data analysis once those rules have been formulated. How will we be able to discover what those true rules are if people are successfuly breaking the rules we created? I think that Dr. Peng made a great answer, which was essentially ""We have to know the rules before we can break them correctly"". I am looking forward to the day when we can see this initial discussion of making a framework for good data analysis comes to fruition. "
document_02,Little by little. Our intro to data science course motivates some of these ideas. I just had a lecture on colors a couple of weeks ago for my data viz class. If interested check: https://www.reisanar.com/slides/colors#1 
document_02,"I really enjoyed how he included music so seamlessly into his lecture. Talking about more abstract idea can sometimes be challenging but being able to draw on peoples' prior knowledge is a great way to drive a point home. I particularly like how he showed the different examples to write down a song. Some are very simple and others are very specific and detailed. Different genres of music require different levels of detail. That lesson is very applicable in many different fields but many people don't talk about it or even think about it. He really stared to make me think about different ways data analysis can be done with different levels of detail, but is detailed enough for someone else to reproduce it close enough to its original form."
document_02,"I also liked how he mentioned the question ""How to determine whether a data analysis is good?"" near the beginning of the lecture. It feels like it is almost impossible to write a formula or list of rules to determine if data analysis is good or bad. It is easy to look at a completed project and see if it is good or bad but hard to write a rubric for an unknown project."
document_02,Overall this was a good lecture and it gave me a lot of good things to think about.
document_02,Glad you enjoyed it.
document_02,"I enjoyed watching Dr. Peng’s video as it had much insight. It also had many interesting points as well. The first interesting point he made was how many data analytics jobs are going to be needed. He noted a study from McKinsey and Company in 2011 that predicted in 2013 there were going to be approximately 140,000 to 190,000 unfilled data analytics job openings. It was also predicted that there are going to be 2.72 million data science and analytical roles in 2020. Another interesting point that was quite intriguing is that every single person is s a data analyst, even without their knowledge since data is always constantly being collected now. A third interesting thing about this video is how Dr. Peng compares data analysis to a bus stop. He tells that a good bus stop is comparable to successful data analysis since it is pleasant and easy to work with, which leads him into the topic of design patterns, which are quite interesting as well. Overall, this video was very intelligent and shed more light on the topic of data analysis."
document_02,"I really liked the presentation by Dr.Roger D. Peng, I thought it was really interesting and really well presented. I think one of the most interesting takeaways from this presentation and maybe one of the most important is that data is taking over the world, but the analysis is far from it and it's getting worse every day. It's really put some things in perspective for me because we have all of these ways of collecting data and accessing huge data sets but the analysis of those data sets is really not that good. This can also be seen in the reflection of the number of jobs that are being increased based on projections. So back in 2011, they projected that by 2018 will be 180k jobs for data science on the market. A new production was made in 2018 that projected by 2020 there will be 2.72 million jobs on the market which is insane change! Something else I found interesting/funny was how he compared object-oriented software development to designing a bus stop it was pretty funny to me but also spot on."
document_02,"I enjoyed Dr. Peng's lecture style and how he used music examples to explain conceptually how interpreting data analysis results should be approached. The ""What Can We Learn?"" slide helps put into perspective how the presentation of data analysis varies, in that it allows us to set expectations about what is ""good"" data, but is not necessarily instill a concrete ""command"" of what is exactly correct or not correct. He then mentioned that his point was that he thinks ""a lot of discussion about data analysis to date has largely been focused on the instruments, and there's not much discussion about the music that's being produced."" I feel this is still true largely because the results of analyses are just laid out and presented as findings, but does not give much insight into the problem or topic being analyzed unless you're familiar with the data and where it came from, which in most cases can be obtained from reading a report, but not solely from graphical output. I found the last closing slides interesting, that it's important to think about what we're trying to achieve in the data analysis, and how evaluating and testing them with different methodologies can affect the results, and even more importantly, how to find the right balance to ethically conduct the data analysis process. This video was thoroughly enjoyable! "
document_02,"I really enjoyed Dr. Peng's lecture and found it really engaging and informative. During his presentation, he did a great job of showing how difficult it can be to assess what a good data analysis is. Since each data analysis is so different from the next there is no such thing as a ""cookie-cutter"" version of data analysis. That also means that there isn't a set of rules for data analysis because there logically can't be with how different each data set is.  This then leads well into seeing how the level of detail of data can completely change how data analysis is done. He made great points by comparing it to music theory and that creating a data analysis theoretical framework would be extremely beneficial to the data science field. I also enjoyed his explanation of re-search and that using different fields information in yours almost seems like new ideas but really they're not at all."
document_02,"I feel like this was my problem back in statistical learning, that the outputs should be somewhat ""cookie-cutter"" in a sense, which is what I was expecting, but this is most definitely not the case. "
document_02,"The ""wait, so what am I supposed to do?"" story for data science...an absolute mood. I feel a little lost sometimes on when data analysis is complete. Is it after summary statistics? After I run a report from the DataExplorer library? After building a model? Data analysis feels like an art form, and the ""end product"" is in the eye of the beholder. Other pieces of this talk in general were reminiscent of the types of data storytelling learning in Intro to Data Science last semester (especially that part that mentioned that a good data analysis needs to guide, not command). I liked the idea that theory was infinitely scalable, and knowing what to expect in order to alert when things are going wrong seems like a reasonable explanation for teaching theory. Especially in a project based school where writing programs are key to good grades, it's a fresh reminder to go back and remember why we're learning."
document_02,"ICYMI: the podcasts suggested are ""Not So Standard Deviations"" and ""The Effort Report."" (Mostly for me to reference later :D)"
document_02,"I always look forward to new episodes of NSSDs, with rockstar Dr. Hilary Parker (former data scientist at @Etsy  and @StitchFix ), and the Effort Report with legend Dr. Elizabeth Matsui (now @ UT Austin)"
document_02,"I find it interesting talking about the job market and the huge demand for data analysts and data science experts. The data revolution is exponentially growing and so is the demand for data related jobs, but it raises the question of if there is enough educated people to actually fill these roles. According to many online resources there is a massive shortage of data scientists for the demand this year and it has been that way the past few years. Since it is one of the fastest growing fields and highly in demand, many schools should probably be creating coursework and programs to support this industry and allow more growth. Machine learning and data science are key in many big companies now and probably will continue to be essential for many years to come with all the power that comes simply from data. It makes one curious what the next big technology revelation will be and what will come in terms of jobs for that."
document_02,"I agree with this post, especially since MIT, a Tech Ivy league university, didn't have a Data Science Undergraduate and/or Undergraduate program. More schools should push for Data Science programs in order to help the demand for Data Scientist in the technology economy."
document_02,"The presentation was interesting, Dr. Peng was a great watch and was very informative. I liked the different stories he had and the one that really resonated was the one about the student not knowing what to do with their data set."
document_02,"""If data analysis is to be well done, much of it must be a matter of judgement and theory, it will have to guide, not command."" I liked how he simplified/explained this. You don’t have to analyze data the same exact way every time, it allows for relative freedom and different possibilities as ""not every town looks the same but they can all follow the design patterns."""
document_02,"I loved how he used music to explain visualization within data analysis. Not only that, but he really used music a lot to describe and really paint different pictures of data analysis. It was a fun way of hearing DA being talked bout."
document_02,"I liked the history he gave of Data Analysis, the ""past futures"" segment,  was informative."
document_02,“Data analysis has been focusing on the instruments and there is not much discussion about the music that is being produced.”
document_02,"I took the time to listen to Dr. Roger Peng's Not-So-Standard Deviation Podcast, and linked it here  on Spotify. I thoroughly enjoy the concept of learning through podcasts on your own time. It's incredibly easy to listen while commuting or doing monotonous work. It gives the listener the ability to find virtually any subject matter expert on a specific topic that they may find interesting or may be seeking beyond surface knowledge on. I listened to episode to episode 116 - Coffee Data Science Mashup  and 117 - Failing in Statistics . "
document_02,"""We could teach data analytics in the way we teach biochemistry."" I thought this was a wild statement that caught me off guard until Dr. Peng explained it, and then directly linked it to Isaac Asimov. He implied that it could taught with an emphasis on what we have learned and relegate what we learn to the detail oriented processes of the lab; which I think is interesting since data science does not have labs in the traditional sense, the lab is where we take it with our devices. Which I think makes our field of study that much more intuitive and flexible, and deserves the distinction over traditional statistics study as Dr. Pen emphasized. So in a way we enter our lab every time we open RStudio and extrapolate data sets, and share our findings through visuals. "
document_02,"I also thoroughly enjoyed his mathematical analysis of music theory, and the timing of notes that follow these theories. Here are a couple of favorite artists that I draw musical inspiration for guitar scales that follow golden rules of musical progression:"
document_02,"Thanks for sharing the links. As I commented in another post, I always look forward to new episodes of NSSDs, with rockstar Dr. Hilary Parker (former data scientist at @Etsy  and @StitchFix , and the Effort Report with legend Dr. Elizabeth Matsui (now @ UT Austin)... I've done this before: towards the end of the semester I'll share some of my favorite podcasts/websites"
document_02,"His presentation was fun and I really enjoyed learning about data analysis, specially because the demand for data science is increasing significantly and it was interesting to think about how everyone is a data analyst, whether we know it or not. Answering the questions ""what is data analysis"" and ""what makes data analysis successful"" is challenging and I liked the way he approached to answer these questions, asking himself what makes random things successful (like a bus stop). Also, I love music (not the best at music theory though) and I enjoyed how he compared data analysis with music. In his example he ""violated"" four rules of music theory and he asked himself how to fix it. The rules tell him what sounds good or bad, and according to them what he created would sound bad. He then compared it to different types of songs, commenting about how detailed and complicated they are, and how the artists know what they are doing, even when not following the ""rules"" (it was fun for me to watch the ableton example because I have been trying to learn how to use and read the software). He comments about how they are not making the same music, but they operate the same way, using the same theory and rules. That was the perfect way to relate to data analysis, and how the rules and theory are important for a good data analyst."
document_02,"This presentation was very interesting and structured well. I think we could learn not only from the information he is presenting, but from his presentation style as well. He used a lot of different visuals including quotes (without boring everyone), drawings, and excerpts of music. He strug all of his slides together with speaking and making jokes, and I enjoyed wathing the presentaion due to this. "
document_02,"My favorite part of the presentation, like many of my classmates, is when he begins relating Data Analytics to music. Even though not everyone can understand sheet music, he explains it well enough for everyone in the audience to understand the metaphor of a counterpoint and that there are rules for what 'sounds good'. It was interesting how he related this back to designing plots  and how they have rules for what 'looks good'. This has given me a new way to think about data in the future. "
document_02,Comparing the chord representation of the song to a distribution was also one of the more interesting parts for me. I had never thought about the parallels between different representations of music and other data.
document_02,"I enjoyed this lecture, especially with students who do well in data mining classes and understand the material but don't know how to apply it in real-life scenarios."
document_02,"Tidy Data, which was coined by Hadley Wickham, served as the foundation for an increasing number of tidy mythologies. It uses multiple commands: mutate, group_by, summarize, rename, spread, gather, unite, filter, and separate. "
document_02,The Design Patterns portion caught my interest as well when he explained how to make a good design with a set of criteria or aesthetics. A freeform theory of construction can be developed which gives essential structure but allow a wide range of possibilities. Design patterns are extracted by observing behavior and application of varying tools. 
document_02,"His suggestion to the book, Grammar of Graphics, would be helpful in my future assignments and projects since it could help me make better plots for my data.  The book is a vocabulary comprehension for describing ggplots."
document_02,"I found it interesting that he went over past example of future data analytics, with many of them being wrong, and examples of his own experience, and came to the conclusion that data analysis is not a well-defined process. Pairing that with the fact that it's a field in such high demand is fascinating. I guess even though data analytics is everywhere and still growing, in some ways we are still in the ""frontier"" days of data analytics, and there is much to de discovered and defined. My experience in coursework matches up with his conclusion that education in data analytics is about methods and applications and doesn't have as much overarching theory and reproducibility as music, programming, and building cities. It will be fascinating to see what frameworks are proposed and how the field progresses going forward."
document_02,"A few small things that I enjoyed: the tone and humor in his presentation, the dig at excel, the unexpected note in the symphony, and the joke about the dance party."
document_02,"For those that may be interested, here's a related article by Roger Peng:"
document_02,https://simplystatistics.org/2018/12/11/the-role-of-theory-in-data-analysis/ 
document_02,"The most interesting point in the video was the urgent demand for Data Analysts and Data Scientists in multiple fields. There are many different challenges related to the scalability of the way people are taught and how online courses could automatically evaluate a good or bad analysis. It isn't easy to differentiate if you can only rely on the machine capability to do it. Furthermore, the reproducibility of papers was also raised since a substantial amount of studies were suffering from this problem. The analysis worked for the paper, but when it came to implementing the results in another study, it was not possible.All these points were addressed, and the complexity of these issues will most likely grow with the rapidly increasing demand for data analysts and data science in the market. It was also interesting to see that analysts focus only on the reproducibility field to retract papers that could potentially have problems."
document_02,I thought that was very interesting too! I didn't realize data scientists are in such high demand in a variety of applications.
document_02,"One thing that I think is really established in this lecture is that data analysis is a rather new field. Sure, data analysis has been happening for as long as humans have been around, whether we realise it or not, but it hasn’t really become a defined field until recently. I think many companies realise they need data analysts but do not know how to do it properly. What Dr. Peng has discovered is a subcategory of data analysis that is like data theory. One point he brought up is that theory can be infinitely scaled. It is general enough that it can be applied to any situation that requires analysis. He described this by comparing it to music theory, which was interesting. Because automation would be very difficult and teaching everyone data analysis near impossible, it is best to just have theory that can be applied to pretty much anything."
document_02,"I found two things really interesting in this video presentation. The first is just how large the field of data analytics is and how fast it's growing. I find it astonishing just how large this section of the industry has become and how every business is demanding someone in the specialization nowadays. I sometimes wonder if the bubble will collapse and if data itself will come out of fashion in the future to be replaced by something else. I don't think it  will happen for a while considering how reliant we are on data right now thankfully. The other interesting thing I took away was what qualifies as a ""good"" analysis of a dataset. It's an interesting problem I never considered because so far my only feedback has been from my professors. It is an introspective question about whether the analysis is good which itself doesn't have simple guidelines to follow. We can get a good sense based on gut feeling if an analysis is good, however there will never be a universal consensus."
document_02,"This was a great presentation, best one yet. I liked that he said everyone is a data analyst. The challenges he highlighted surrounding data analysis were also pretty legit. Personally, even when learning in class I cannot quite tell whether I get the concepts until I have to practice using a particular tool then realize it makes sense.  I also always struggled (and I guess I still do sometimes) explaining to people what exactly data analysis is whenever they asked me for clarification.  That music example was also cool. I think that was my favorite part. I am probably a melomaniac so it was nice to see music being incorporated in a data analysis talk; because I would have never thought that would happen "
document_02,Some of the points I found interesting:
document_02,This lecture was very engaging and a different look on how to view the future of data analytics.
document_02,"Dr. Roger D. Peng's lecture was very informative and well-paced. I thought it was very entertaining how he mentioned that students end up with the skills to do data analysis, but have no clue what to do with the processed data. He focused most of the discussion on the need for criteria of achievement with data analysis. He compares criteria of achievement in other fields to demonstrate this need. In essence, he raises the question What is the rubric for determining how good a data analysis it? He concludes that he doesn't know the answer, and that ""it can't be I know it when I see it."" He suggests some potential criterion for evaluation such as reproducibility, translatability, robustness to new data, and communicability. These are more broad categories that include more sub-components, the full breakdown can be found near the end of the lecture."
document_02,"Another interesting point from the lecture: ""Rules are guidelines, not demands."" Something doesn't have to follow the rules perfectly to be good or even great, they simply provide direction."
document_02,"I've heard about the reproducibility crisis in the past. I think a simple solution to combat the crisis is to make all information open and available on GitHub with strong documentation and available data to support the study. This would allow for others to re-run the study, and perhaps even modify and improve the study."
document_02,I really liked hearing about the origins of ggplot (grammar of graphics) and tidyverse (tidy data). Up until watching this video I just assumed the names were made up!
document_03,"CRAN is a website, founded in 1997, where all the packages are available people used to extend R. CRAN actually makes R unique by availing all those packages available to public.  "
document_03,"Apparently, I got the history of evelovement of Rstudio, released in 2011, is the most exciting one since we are currently using this in our coursework. R studio is really a good collection of useful open source package, already mentioned by Professor in the class, and nice example of acceleration and adoption of R."
document_03,"I never knew about the connection between R, S, and Macintosh, so that was an interesting part of the video. It always amazes me how many different software come to be because of Macintosh during the 1990's."
document_03,"Just as David Smith mentioned I was surprised by the similarities between R and CRAN from 2000 to today, I suppose by this point the R and CRAN esthetic are mostly for symbolic reasons rather than necessity. Even Rstudio looks almost identical from 2011 to today."
document_03,"When he mentioned Microsoft's connection to R, I was interested in the R integration into SQL Server and other Microsoft products. I must have been living under a rock because I never knew about the the service within SQL Server (More information found here ). I'll have to continue doing some research to see better how these two services can be ran together."
document_03,"The graph he provided regarding the number of R packages was interesting to see, the slowing growth of R packages was surprising."
document_03,"It was interesting to see R from this historical perspective, within my circle of friend both at work and personally R is still seen as an emerging technology. This video puts R into a new light showing its maturity. It will be interesting to see in the future if R continues to develop at an increasing pace or if something new will replace it."
document_03,"That SQL Server and R integration sounds like it would be useful for the board game database we're working on, given that it's supposed to be a platform for analysis."
document_03,"The hypothetical evolution from interface to domain specific language to fully featured programming language is interesting. It makes some design decisions clearer, and it's impact can be felt today. It also makes it clear that R isn't just the language itself, but the entire ecosystem of libraries and environments."
document_03,"How did R switch from S being primarily implemented in Fortran to C/C++? I found some graphs about the code base here . I imagine at the time it was hard to justify adding support for an entirely new language of libraries, but maybe the languages are sufficiently interoperable for that not to be a concern."
document_03,"I had no idea that RStudio was such a recent development. What was the environment of choice before RStudio, especially considering that RMarkdown didn't exist? Did people just spend a majority of their time in the REPL command line interface? That sounds much less productive."
document_03,"           The interesting part is the CRAN, which is organized in 1997. It is a comprehensive R Archive Network of ftp and web servers worldwide that store identical, up-to-date versions of R's code and documentation. This is open-ended; we can access all the codes and packages from the initial development to today's upgrades. There is a drastic growth in the development of the new package, which includes statistical analysis. In our class, we used this to install R and then install multiple packages like tidyverse (an opinionated collection of R packages designed for data science), purrr, dplyr etc.  "
document_03,"           The R's history, including its predecessor-S and plenty of updates in new versions in R, is a gigantic development. R growth in academic and research communities is exciting. Even I am not familiar with the relation between S, R, and macintosh. I got a clear idea about them after watching this video, which is really interesting."
document_03,"            I got this video(https://www.youtube.com/watch?v=z1vTSdRolgI); it has R releases' history since 1997 and discussing the R Core Team's role, emphasizing development principles, and release management issues.  And also, the choice of release names since 2011 are included."
document_03,"          Also, R is the best platform for the machine learning approach. I got another video, the Machine Learning with R and TensorFlow (https://rstudio.com/resources/rstudioconf-2018/machine-learning-with-r-and-tensorflow/), describes the complete workflow, including data ingestion, training, and deploying models into production. Where TensorFlow is an open-source software library using data flow graphs for numerical computation. TensorFlow is created to perform machine learning and deep neural network research by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization, and now it is applicable in a wide range of other fields as well. "
document_03,"        Finally, in 20 years of R, there is a significant development, and it will be great to see in the future if R continues to expand at a growing rate."
document_03,"There were lots of insightful parts of this talk. For starters, I wasn’t entirely familiar with how open source projects were maintained, managed and assimilated into developer’s lives. With this video I got a good glimpse of how open source software is viewed by companies. This to me was a surprise. You’d intuitively think that corporations would love the fact that a software is free (since they don’t have to pay for an expensive license). The fact, however, is that reliability and reputability play a much bigger role than I expected in these scenarios. After Microsoft purchased Revolution Analytics is when companies really began assimilating the language into their structure. Like others mentioned too, I was surprised to see how new R studio was, and what an experience must have been coding without it. Overall, I loved the talk and seeing the growth and development of R. I believe every and any contribution to open source software is to be cherished and appreciated."
document_03,"You have made a very good observation, that having a brand named company backing/supporting a product or being associated with a well known entity legitimizes (adds reputability to) the product. Also, David Smith was with Revolution Analytics at the time Microsoft obtained it, and he said that he has been with them since, and this makes his YouTube video presentation a reliable source for learning about R's history and technology."
document_03,"On the other hand, open source software that has an enthusiastic community is arguably more secure and better supported than proprietary software."
document_03,"It’s neat how so many programming languages were born out of necessity instead of trying to reinvent the wheel. I didn’t realize it was developed as a mac alternative to a preexisting software. Some things never change, like the CRAN homepage apparently. The fact that the first 1.0 release of R works on the current build of Windows 10 is quite impressive. I’m surprised R studio wasn’t released until 2011. The fact that R had been integrated into Microsoft services definitely helped it grow and be solidified as a legitimate option. The number of packages has been steadily increasing up to 2017, there was a decrease in 2018 which I would like to learn the reason behind. There’s a wide variety of fields that R is used in, I can see how it would be especially useful in managing the data from the covid 19 pandemic outbreak. Before this class I had never heard of R, so I’m glad I have been exposed to it and it’s many applications."
document_03,"Yes, the R programming language is awesome! I very much enjoy the data and statistical computations and capabilities that it can perform. I can use R for my scientific and research reports, especially for my future thesis. R is a high level programming language that is continuously being upgrading by improving previous packages and having newly created packages being added to its library. The future of R programming seems to be a fruitful one for the R designer, engineer, and user. I very much enjoy learning to use this programming language."
document_03,"I can imagine the drop-off in R package support being the natural fall in an exponential growth cycle, seeing as by 2018-19 there were probably so many packages for R that it became redundant to continue producing more, and made more sense to simply update existing packages with newer functionality."
document_03,"I don't have much to say on the technical side of R's development; it seems like everyone else has something to say about that; I'm more interested in this idea that R started from just two guys, a simple frustration (no S for Macintosh computers), and a piece of proprietary software for which there was an incentive to imitate and remake into something open-source, and that would eventually come to replace the original product in circles where it was once dominant (sound familiar?). Making something that would practically replace S wasn't like the original scope of it, apparently (it appears that R was *just* for Macintosh, initially), but it doesn't seem like it took long for that to change."
document_03,"Why I find that interesting is mainly for how common the story seems to be. Mac after all, had a similar origin, but it turns out with this that it was just simply noticing a problem that spurred its development, as well as filling in a kind of niche that others had not, and that was still clearly growing given that digital data computation and analysis now is far more powerful than it was back in 1993. It shows by just how Smith implied a floating-point unit as being this thing computers at the time either had or did not, even though it's hard to imagine a lot of scientific computing without it now."
document_03,"I agree that the humble origin stories of various languages/software products is pretty crazy to think about in retrospect. I think it's definitely a testament to the idea that every great idea always has to start somewhere, almost always before any huge amount of funding or other people believing they could actually accomplish something big."
document_03,What did I enjoy about this presentation?
document_03,"I enjoyed learning about the very detailed history of R’s development, because I can see that R is a very important statistical computational software that has made enormous amount of contributions in data science and integration of technological media. I enjoyed that this video was very informative about R’s implementations within other software environments, such as RStudio. I also very much enjoyed that David Smith was very passionate and enthusiastic about the topic, which made the video more interesting."
document_03,What caught my attention?
document_03,"The things that caught my attention were: 1) Even though 20 years has passed, the R version 1.0.0 still works on Windows 10. The only problem is that it is not able to access packages because the needed links and “wget” to access the internet is not supported. 2) R is not a for profit company, but still manages to receive donations to fund servers for CRAN, R Journal (13 years of publishing thus far), and useR conference. 3) Strangely, on January 6, 2009, which was David Smith’s birthday, was also the same day that he went out to buy the printed newspaper and saw that the New York Times published an article about R, which was printed on the entire, front page of the technology section, which was also a four page story."
document_03,"Is there any comment and link to anything I explored, such as: integration with other products, funded projects, applications, or even other discussions, after watching this presentation? "
document_03,"Yes, I found a Wikipedia webpage that states that another statistical programming software, Statistical Product and Service Solutions (SPSS), that was first developed in 1968 has been upgraded, to version 14, to implement the Python programming language, which then allows the SPSS software to access and run R packages. This makes the R software even more valuable, since, like David Smith mentioned in the video, that the more uses for R, the more likely it will continue to be used in the future. Here’s the link to the Wikipedia webpage: https://en.wikipedia.org/wiki/SPSS "
document_03,"    This talk was very enlightening on what R is at its core and its history. It's pretty amazing how long R has been in development and in use, beginning in 1992 and its predecessor S being made in 1976. R being the mac version of S with an optional floating-point and 2.5 MB of memory is also quite funny. finally R's official 1.0 release was quite some time after the initial build having released in 2000/02/29 around 8 years after.     All in all, it is very plain to see how much the developers and people who use R a lot have such a love and passion for the language. It being open source and non-profit has, even from the very beginning, spawnd a lot of packages and forks for it made with the love and passion of the community. We use some of these in class with tidyverse and ggplot, and CRAN has made it possible for people to pass these on easily to others and to use the older versions of R for compatabilityy. While some of the older OS's no longer are supported in newer versions they kept the support for the old OS's for a while (95/98 ending in 2008, 2000 ending in 2011, and XP ending in 2017 for the windows version.) The scale of releses for R is interesting with version 4.0 relesing in april of this year it went from just a rough idea in 92' to a 1.0 release in 2000 to 20  years later a 4.0 release. My main take away from this presentation by Mr. Smith and the presentation of Mr. Wickham is the passion that devs and users have for the language and how much they put into it for the benifit of the language and others."
document_03,"I always found it fascinating how fast technology has improved in the past few decades. Looking at parts of its history, R is shown to have greatly contributed to these advances with its availability and ease of use. Since S was only commercially available, it may have not been feasible to use for most scientists to use as a tool. Also, its limitations to only being used on Windows computers and its incompatibility with Macintosh computers put further limitations on which devices are needed to develop, use, and expand this language. R has overcome that barrier by being compatible with Macintosh computers and also being open source. Since then, R's growing community has caused it to be one of the top languages for statistical computing. With more developers and an increasing amount of packages being developed, more tools are being released for scientists to use less time on coding and more time analyzing."
document_03,"One of the aspects that I didn't know too much of the backstory about was about Microsoft supporting R through their aqusition of Revolution Analytics in 2015. This was a pretty huge step for R to get more reputability, and to entice other companies to look at it more as a tool for their own projects now that the biggest corporation had taken an interest. Furthermore a lot of relevant products that I have used over the last couple years are a result of this initial support that I didn't realize, such as the Azure services and Power BI."
document_03,"I also had not heard of the R Consortium but it was really nice to learn that big companies were interested enough in an open-source language to maintain steady investment. From this Consortium, investments were made in to R-ladies  which I wasn't aware of but thought it was a really good outreach program to promote gender diversity and encourage more women to promote and collaborate worldwide. "
document_03,"  Dave Smith's presentation provided fascinating insights into the R project's beginnings and its relation to the S language that preceded it. Since S was a commercial software and was not available for Macintosh, Ross Ihaka and Robert Gentleman started working on R language from a small lisp interpreter and extended its data structure with atomic vector types and altered its evaluation semantics and made it look like S, but this statical software is used on Macintosh.  It is also interesting to see the future goals of the R project as envisioned back in 1998: ""to produce a free implementation of something 'close to' version 3 of the S language""; ""development of an integrated user interface""; ""to get substantial use out of R for statistical work and teaching."" according to a paper I went through (https://www.stat.auckland.ac.nz/~ihaka/downloads/Interface98.pdf) "" R began as an experiment in trying to use the methods of Lisp implementors to build a small testbed which could be used to trial some ideas on how a statistical environment might be built."" after technically 20 years of the official release of R, I see that the R project has succeeded beyond expectations. "
document_03,"I know everyone is talking about how they never knew that R was a dialect of S, but if I'm being honest, I had never heard of S before this class either. I remember that Professor Rei briefly mentioned it during the first week or so but never really looked into it until now. I think the most interesting thing is that R was initially developed to do the same thing as S, except, on Macintosh devices. The reason I think this is interesting is that R seems to have completely eclipsed S in terms of popularity. So much so that when I looked up the wiki article for S it says that R IS the ""new S"" along with something called ""S-plus"". From what I found, S-plus is just a (less popular) commercial version of S that is available for purchase. I would assume the reason that it is less popular is that its competition, R, is free and open-source, which is kind of hard to beat. Anyway, I know a lot of people used this as a point of discussion, I just noticed that no one was really mentioning that even though it started as a small branch of S, it grew into something much bigger, and I thought that was pretty cool!Also, it's pretty easy to tell which language is most popular based on the length of the Wikipedia page:"
document_03,Shttps://en.wikipedia.org/wiki/S_(programming_language)  
document_03, Rhttps://en.wikipedia.org/wiki/R_(programming_language) 
document_03,S-plus (So bad even the link doesn't work lol)
document_03,https://en.wikipedia.org/wiki/S-PLUS
document_03,"One of the things I found interesting was that R was developed as an open source alternative to S, which was made from Bell Labs. It continues to amaze me how many programming languages and pieces of software seemed to originate from Bell. Getting to see the relatively simplistic reveal of R as just a tool for a Macintosh lab and some research work kind of reminds me of the origins of the Linux kernel. Smith's mentions that companies were initially hesitant of R due to it's open source nature also seems to be reflecting a trend of companies becoming more open to using free software. While I knew that R had a large number of packages available for it, seeing a visualization of the growth in releases was very impressive, and it seems like community development for R has really exploded over the past decade. The fact that the 1.0 release is also able to run on modern versions of Windows is pretty amazing, as there is so much software that runs into compatibility issues as operating systems update and new CPUs release, yet R seems to have been stable for 20 years."
document_03,"For me, the most interesting part about the development of R has been how it has thrived through being a free and open-source piece of software. To this day, anyone can download R and R Studio at  no cost and have access to some of the most powerful and widely adopted tools in the field of statistical computing and data analysis. This appears to be one of the most important aspects of R that separates it from its competitors and why it has so quickly outpaced its progenitor, S. The ease of access and use means that users from an array of disciplines can make changes and create packages that allow R to morph, grow, and evolve  quickly. This open-source development ecosystem is what has allowed R to dominate and remain relevant for so long."
document_03,"R proves that powerful software doesn't have to be locked behind a paywall and intellectual property laws to become a financial and critical success. In its commitment to remaining accessible at no cost and open development it embodies the ideals of the GPL, which states that free-software means ""free as in freedom."""
document_03,"First off, I want to start by saying that I think it is really interesting that the main reason R was created was because S (which was the first data analysis software) was not made available on Macintosh. Two people decided they need to do something about this problem and they created R.  I also really like how CRAN which is a public website for all things R related (actually downloading R and a repository for all the packages that people contribute to extend R). It makes it easier for people new to R to really see what R offers."
document_03,"I totally understood what David Smith was saying about how people did not really trust R because it was a free software that was available to anyone. People were not sure who will support R and what was going to happen but when Microsoft got involved, all those worries went away. I truly believe that everyone behind the development of R truly just care for the future and want everyone to learn it without having to worry about anything. I think that is why they also get donations all the time and can still work and update and develop R in the future."
document_03,"Coming from a mechanical engineering background, the very first time I had ever heard of “R” was from two friends of mine at poly who were learning it around a year ago! Overall, Mr.Smith’s discussion highlighted several points of the history of R and how it has come so far from an Idea maybe 25 years ago!"
document_03,"It was interesting to see that before R, there was S that is almost 45 years old and basically twice as old as the official release of R (being its predecessor). Given that I had only experienced R this year, I really had had it in my head that R was some sort of new programming language (as I had already known about C++, Python, Java, etc…) The fact that R isnt a locked software that you have to pay for surprised me; and i’m glad that there is a community that gathers financial support to help upcoming developers of this software and programming language! It was interesting to see that R was meant to make the transition of the S programming language to Mac computers; and how R is available on both Mac and Windows today!. CRAN was interesting to see that is the depository for all the R packages, who’s UI has not really changed over the years, despite having probably more than hundreds of thousands of visitors since its inception!"
document_03,"As I was watching I found it very interesting in how he discussed the history and growth of S and how R has been developed. One of the more interesting things I found was when Microsoft acquired Revolution Analytics and suddenly R blew up. I'm not sure if it is because of the acquisition or just that more people would use R, but personally it intrigues me how people jump ship to R in that volume. I also find it odd in the ratio of new packages to the edits of existing packages. With new technologies being introduced and maybe not have been thought of from the beginning of R, I would think this would create a higher demand for new packages specific to these new 'problems' or demands. Though I have not done much research on that statement and it seems R is more for data analysis as mentioned many times in the presentation, R seems to have its place and will continue to grow and mold itself for the future."
document_03,"To start this, I actually find the entire history and timeline of R quite interesting. I haven’t really looked into R before thus the whole video was uncharted territory for me. The actual start of ‘S’ in 1976 as a precursor to R was an intriguing foundationally start, it literally used Fortran posts. ‘S’ was used to access all of the computational libraries that would eventually be updated and used in R. Then 24 years later in 2000 R 1.0.0 got released. This version of R also can be downloaded and ran even today, 20 years later. Which, to me, is just so cool to see that this software version actually held up after all these years and can still run. Then in 2009 R actually ended up hitting the front page of New York Times which just shows its success and use in companies. Jump two more years forward to 2011 and RStudio was released to the public. Then in 2015 one of the biggest tech companies, if not the biggest tech company, helped support and even further build R and the R community as a whole. As we jump to 2019 we see the CRAN package growth to continue to increase as well as the development over the last couple of years. As we approach today, R is still being used, even helping with chronavirus projections which just shows its incredible impact. "
document_03,"It's always funny to see just how rudimentary computers were 30 years ago.  He mentions the email in which Ross Ihaka specifies the hardware requirements for the early R release and includes the statement ""floating point is preferable."" The initial release was also 2.5 MB, which is less than memory than the average song. Dave talks about the period when R is becoming more mainstream. In 2009 a NYT article comes out talking about the increasing importance of R. At that time, just one repository (not named in the article) had already close to 1,600 packages. Interestingly, the author of the NYT article, Ashlee Vance, got a statement from a director at a company that makes proprietary software (SAS) in direct competition to R where this director said, ""We have customers who build engines for aircraft. I am happy they are not using freeware when I get on a jet."" Besides being a hilariously belittling statement, it also just wrong. Open source software is often better than the proprietary alternative, in terms of security and support. The article also includes a picture of Ross Ihaka, which I really liked. It's Ross holding a small whiteboard with a code snippet demonstrating the scoping rules for R, and showing a function being declared within a function. Overall just a really nice presentation by Dave."
document_03,NYT article link here:
document_03,https://www.nytimes.com/2009/01/07/technology/business-computing/07program.html?partner=permalink&exprod=permalink
document_03,"I found it interesting that there is a lot of history of R.  I did not know that S was the precursor to R. So far with what we have done in class, you can tell that R is older language. Many of the shortcuts and ease of use in other programs are not in the base program of R. I am glad that the creators wanted to make it an open source program and that has continued on. I did not realize that CRAN was repository for R packages.  It is shocking that it took RStudio to be released to the public, if it was open source. It seems that they would want their code to be able to be utilized by as many as they can. I did not know that R was owned by Microsoft. It is quite sad that for R or any language not to be considered good, unless they are owned or backed by a major corporation. "
document_03,"I thought this video was really interesting. I did not realize that R had a predecessor called S. In the video David talks about how S was created in 1976, and when it was originally created it was not compatible with Macintosh. This was essentially the motive for creating R."
document_03,"Another part of this history that David talked about that I thought was interesting was about CRAN, and the website. A couple things stood out to me like; it has not really changed over the past 20 years, and when David compared the CRAN to an ecosystem. His reasoning behind this comment was the ability to extend R through packages, and the ease of access and distribution to them. David then uses the CRAN to get R 1.0.0, and I thought that was kind of cool to see how far it has come, not just as a language, but also the creation of RStudio."
document_03,"In this week's video for discussion, Dave Smith presents some interesting information about the history and development of the R language. I think it is really cool that the original developers initially created it as a solution for using S on macintosh computers in 1993. Over the next several years with continued development, it became more mainstream in 2009 - 2011 when RStudio was initially released. Dave goes on to talk about how Microsoft now supports and incorporates R into many of their software platforms. This was something that I had no idea was happening - I was under the impression that R was used for more independent purposes. I think it would be really interesting to continue studying the mechanics of R so that I may one day work with those implementations.Finally, Dave mentions how R is now being used at the forefront of analytical science. This came as no surprise to me, as that is clearly what R is all about, but this reminder further solidifies my interest in developing my skills in R. "
document_03,"At the beginning of the presentation I thought it was really interesting to know that R stated out as a Mac alternative to S. I always think that type of intuitiveness is fascinating, and how ideas people think up can far exceed expectation, and morph into something new. Knowing more about the CRAN website and that to download R before its release was so different to expectations nowadays. I also thought that it was really funny that it predates the Way Back Machine, so finding an archive is difficult."
document_03,"Something that I actually found very funny, and was mentioned only as a footnote, was that R was Y2K compliant. Just seeing that note, just made me remember how much hoops computer and electronic companies had to go through to appease people over the Y2k scare."
document_03,"Lastly, I was surprised to know how long it took R to really become a standard. It's jumpstart to the public eye in 2009, and the public release of RStudio two years later was something I was not expecting. I also thought it was very insightful to learn how companies view languages like R before they are considered ""legit"" and how it can be seen as a gamble"
document_03,"I first wanted to note the comparison to SAS and how R is still going strong today as being a great point to the validity of R as a tool in the industry of data analysis. I did not at first realize how many systems and other large projects were reliant on R or data sets which have been analyzed by R to be functional today. Another thing that I found to be interesting and impressive was the nearly exponential growth in usage and package release and support of R which has curtailed in recent years, however still remains as an impressive marker for how popular and useful the R suite is today. I was not aware of how long R has been in use, and I agree that as more and more companies, especially Microsoft began to adopt R it seems to have become more legitimized, as I first heard of R in my freshman year statistics course when my professor mentioned it as a tool we could use, as that was around the time of the Microsoft purchase of Revolution Analytics as mentioned in the video. Here's a link to an article from a few years ago which compares the growth of R to Python: https://stackoverflow.blog/2017/10/10/impressive-growth-r/ "
document_03,"I love how Robert and Ross decided that since ""S"" did not exist on Macintosh that they took it upon themselves to port it to that platform. Personally, I prefer Macintosh laptops compared to Windows laptops so having the ability to use the software that ultimately began from these two 'gentlemen', pun intended, is quite satisfying. I also like the product release naming that R has had over its 20-year history. I enjoy when tech people put a spin on something rather traditional as numbers. It was interesting to learn about Microsoft's acquisition of Revolution Analytics. Microsoft's Azure platform has become a huge player in the digital computing space and it is cool to think that their framework utilizes some aspects of R. Another cool note I would like to point out was the R Consortium. It is great to hear about such a network of people who actively engage in giving of their time, efforts, and donations. Having the ability to ask for funding on huge expensive project ideas is fantastic. And lastly, it was interesting to learn how John Hopkins University utilized ""R"" for their COVID statistics tracking. Earlier this year when the shutdown went into effect I downloaded the COVID Dashboard app that was provided by The Center for Systems Science and Engineering (CSSE) ay JHU."
document_03,"While watching the video, I found a few topics discussed very interesting."
document_03,"First is the language known as S, being a language similar to R. It was invented by John Chambers and was designed as a tool for people doing data analysis that wanted easier access to the mechanical gears in data science with the ability to connect to powerful libraries that fit the user’s desires. It was invented in the year 1976, which shows how far back programming goes."
document_03,"In 1992, the offspring of the S language, the R programming language was born. It was not the first language to improve on S, as there were different versions of S that could be seen as R’s older brothers. But R is the most recognized dedicated Data Science language to day, thanks to it being the answer to a lack of such data science tools lacking on the Apple Mac."
document_03,"Eventually, R took off in 1995 as one of the major languages for programming when it was released under GPL. Later CRAN would be founded in 1997. R would eventually be put into the headlines of New York Times for its popularity in Data Science. It’s really interesting seeing how it took almost two decades for R to become as mainstream as it was in 2009."
document_03,I really enjoyed the video on 20 years of R. I never knew about the R programming language until I took this course. I thought it was interesting that R started as a programming langue known as S. I think that R being a free software has helped it a lot but I didn’t realize that it also was held back by the fact it was free.. It seems like the support from Microsoft really pushed R into popular use. It is interesting how a single large company can cause something to become so successful. I like the R studio and cannot imagine using R without it. I think it was a very good addition the R programming language. This made R much easier and accessible for people like me. I think it has helped me in learning R. Having a good IDE makes a lot of difference. I think the community behind R is very strong and that has really pushed development and growth with R. It’s great that the community is supporting the language and making useful packages for practitioners to use. I think it is interesting that R is being used for Covid Research I found an article about a Covid package released for R https://analyticsindiamag.com/coronavirus-package-in-now-available-for-r-programming-language/
document_03,"I found it interesting how R has been public for over twenty years now. Prior to taking this class I definitely knew of the existence of R but I would have never guessed that it has been in development for that long. Additionally, I would also have never guessed that from this information that R studio was only half the age of R! I also found it interesting how long it took for R to become “legit”. Nine years had passed before R hit the front page of the NYT technology section, and another six years passed before Microsoft started support for R. And that is not because nothing new happened in the R development scene, it was actively being contributed to  for the entire time! Another interesting thing I found on the R website is that much of the code written for S runs unaltered in the current version  of R."
document_03,"The video entails the history of R and it starts talking about how the community of R started and all of this was interesting, I've never seen nor heard of the start of a programming language . They start out by talking about how the coding language started being derived from s programming language. S is the predecessor to R and was started in 1976, then came R in 1992. This wasn't available for mac when it started and the creators came from the University of Oakland. People were trying to find out if S was available for mac but the creators of R took the opportunity to make their program compatible. This video also showed how an open source program sustains itself, R is free and highly successful and viewed as such by companies. It's interesting that R just seems to have started as an alternative to S for mac devices."
document_03,"Firstly, I didn't conceptualize how old R was in relation to the internet before this video. When the presenter showed that the R website predates the wayback machine. I was astonished. As a relative newcomer to the internet, apparently, its beautiful how so many people have came together to build things online - for free, apparently. I asked myself ""How do programming languages like R make money?"" and the lack of monetary compensation in programming language development was a pleasant surprise to me. It seems as though the entire programming community is violently opposed to payment to the programming language - and that's wonderful. That's so pure that humanity can make great things like Java, R, Python, etc. without money as the motivation. Of course, you're resume would be star-studded and you'll definitely make money off of a book, but I'd like to think that wasn't the motivation for every hand that touched R's repository. I'd like to think a lot of people contributed to R to, purely, push humanity forward - beautiful. "